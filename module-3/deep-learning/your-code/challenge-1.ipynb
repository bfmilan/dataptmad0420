{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Tic Tac Toe\n",
    "\n",
    "In this lab you will perform deep learning analysis on a dataset of playing [Tic Tac Toe](https://en.wikipedia.org/wiki/Tic-tac-toe).\n",
    "\n",
    "There are 9 grids in Tic Tac Toe that are coded as the following picture shows:\n",
    "\n",
    "![Tic Tac Toe Grids](tttboard.jpg)\n",
    "\n",
    "In the first 9 columns of the dataset you can find which marks (`x` or `o`) exist in the grids. If there is no mark in a certain grid, it is labeled as `b`. The last column is `class` which tells you whether Player X (who always moves first in Tic Tac Toe) wins in this configuration. Note that when `class` has the value `False`, it means either Player O wins the game or it ends up as a draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps suggested below to conduct a neural network analysis using Tensorflow and Keras. You will build a deep learning model to predict whether Player X wins the game or not.\n",
    "\n",
    "## Step 1: Data Engineering\n",
    "\n",
    "This dataset is almost in the ready-to-use state so you do not need to worry about missing values and so on. Still, some simple data engineering is needed.\n",
    "\n",
    "1. Read `tic-tac-toe.csv` into a dataframe.\n",
    "1. Inspect the dataset. Determine if the dataset is reliable by eyeballing the data.\n",
    "1. Convert the categorical values to numeric in all columns.\n",
    "1. Separate the inputs and output.\n",
    "1. Normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Read tic-tac-toe.csv into a dataframe.\n",
    "df = pd.read_csv('tic-tac-toe.csv')\n",
    "# 2. Inspect the dataset. Determine if the dataset is reliable by eyeballing the data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 958 entries, 0 to 957\n",
      "Data columns (total 28 columns):\n",
      "class    958 non-null int64\n",
      "TL_b     958 non-null uint8\n",
      "TL_o     958 non-null uint8\n",
      "TL_x     958 non-null uint8\n",
      "TM_b     958 non-null uint8\n",
      "TM_o     958 non-null uint8\n",
      "TM_x     958 non-null uint8\n",
      "TR_b     958 non-null uint8\n",
      "TR_o     958 non-null uint8\n",
      "TR_x     958 non-null uint8\n",
      "ML_b     958 non-null uint8\n",
      "ML_o     958 non-null uint8\n",
      "ML_x     958 non-null uint8\n",
      "MM_b     958 non-null uint8\n",
      "MM_o     958 non-null uint8\n",
      "MM_x     958 non-null uint8\n",
      "MR_b     958 non-null uint8\n",
      "MR_o     958 non-null uint8\n",
      "MR_x     958 non-null uint8\n",
      "BL_b     958 non-null uint8\n",
      "BL_o     958 non-null uint8\n",
      "BL_x     958 non-null uint8\n",
      "BM_b     958 non-null uint8\n",
      "BM_o     958 non-null uint8\n",
      "BM_x     958 non-null uint8\n",
      "BR_b     958 non-null uint8\n",
      "BR_o     958 non-null uint8\n",
      "BR_x     958 non-null uint8\n",
      "dtypes: int64(1), uint8(27)\n",
      "memory usage: 32.8 KB\n"
     ]
    }
   ],
   "source": [
    "# 3. Convert the categorical values to numeric in all columns.\n",
    "# 4. Separate the inputs and output.\n",
    "# 5. Normalize the input data.\n",
    "ttt_dummies = pd.get_dummies(df)\n",
    "ttt_dummies[\"class\"] = ttt_dummies[\"class\"].astype(int)\n",
    "ttt_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class', 'TL_b', 'TL_o', 'TL_x', 'TM_b', 'TM_o', 'TM_x', 'TR_b', 'TR_o',\n",
       "       'TR_x', 'ML_b', 'ML_o', 'ML_x', 'MM_b', 'MM_o', 'MM_x', 'MR_b', 'MR_o',\n",
       "       'MR_x', 'BL_b', 'BL_o', 'BL_x', 'BM_b', 'BM_o', 'BM_x', 'BR_b', 'BR_o',\n",
       "       'BR_x'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ttt_dummies.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = ttt_dummies[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TL_b</th>\n",
       "      <th>TL_o</th>\n",
       "      <th>TL_x</th>\n",
       "      <th>TM_b</th>\n",
       "      <th>TM_o</th>\n",
       "      <th>TM_x</th>\n",
       "      <th>TR_b</th>\n",
       "      <th>TR_o</th>\n",
       "      <th>TR_x</th>\n",
       "      <th>ML_b</th>\n",
       "      <th>...</th>\n",
       "      <th>MR_x</th>\n",
       "      <th>BL_b</th>\n",
       "      <th>BL_o</th>\n",
       "      <th>BL_x</th>\n",
       "      <th>BM_b</th>\n",
       "      <th>BM_o</th>\n",
       "      <th>BM_x</th>\n",
       "      <th>BR_b</th>\n",
       "      <th>BR_o</th>\n",
       "      <th>BR_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TL_b  TL_o  TL_x  TM_b  TM_o  TM_x  TR_b  TR_o  TR_x  ML_b  ...  MR_x  \\\n",
       "0     0     0     1     0     0     1     0     0     1     0  ...     0   \n",
       "1     0     0     1     0     0     1     0     0     1     0  ...     0   \n",
       "2     0     0     1     0     0     1     0     0     1     0  ...     0   \n",
       "3     0     0     1     0     0     1     0     0     1     0  ...     0   \n",
       "4     0     0     1     0     0     1     0     0     1     0  ...     0   \n",
       "\n",
       "   BL_b  BL_o  BL_x  BM_b  BM_o  BM_x  BR_b  BR_o  BR_x  \n",
       "0     0     0     1     0     1     0     0     1     0  \n",
       "1     0     1     0     0     0     1     0     1     0  \n",
       "2     0     1     0     0     1     0     0     0     1  \n",
       "3     0     1     0     1     0     0     1     0     0  \n",
       "4     1     0     0     0     1     0     1     0     0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = ttt_dummies.drop(columns = 'class')\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Neural Network\n",
    "\n",
    "To build the neural network, you can refer to your own codes you wrote while following the [Deep Learning with Python, TensorFlow, and Keras tutorial](https://www.youtube.com/watch?v=wQ8BIBpya2k) in the lesson. It's pretty similar to what you will be doing in this lab.\n",
    "\n",
    "1. Split the training and test data.\n",
    "1. Create a `Sequential` model.\n",
    "1. Add several layers to your model. Make sure you use ReLU as the activation function for the middle layers. Use Softmax for the output layer because each output has a single lable and all the label probabilities add up to 1.\n",
    "1. Compile the model using `adam` as the optimizer and `sparse_categorical_crossentropy` as the loss function. For metrics, use `accuracy` for now.\n",
    "1. Fit the training data.\n",
    "1. Evaluate your neural network model with the test data.\n",
    "1. Save your model as `tic-tac-toe.model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(718, 27)<class 'numpy.ndarray'> \t (718,)<class 'numpy.ndarray'>\n",
      "(240, 27)<class 'numpy.ndarray'> \t (240,)<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# 1. Split the training and test data.\n",
    "x_train, x_test, y_train, y_test = [np.array(elemnt, dtype = float) for elemnt in [x_train, y_train, x_test, y_test]]\n",
    "\n",
    "print(f\"{x_train.shape}{type(x_train)} \\t {y_train.shape}{type(y_train)}\")\n",
    "print(f\"{x_test.shape}{type(x_test)} \\t {y_test.shape}{type(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a `Sequential` model.\n",
    "# 3. Add several layers to your model. Make sure you use ReLU as the activation function for the middle layers. Use Softmax for the output layer because each output has a single lable and all the label probabilities add up to 1.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile the model using `adam` as the optimizer and `sparse_categorical_crossentropy` as the loss function. For metrics, use `accuracy` for now.\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 718 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      "718/718 [==============================] - 0s 450us/sample - loss: 0.6856 - acc: 0.5571 - val_loss: 0.6205 - val_acc: 0.6875\n",
      "Epoch 2/50\n",
      "718/718 [==============================] - 0s 137us/sample - loss: 0.6256 - acc: 0.6435 - val_loss: 0.5946 - val_acc: 0.7000\n",
      "Epoch 3/50\n",
      "718/718 [==============================] - 0s 90us/sample - loss: 0.5955 - acc: 0.7047 - val_loss: 0.5815 - val_acc: 0.7708\n",
      "Epoch 4/50\n",
      "718/718 [==============================] - 0s 137us/sample - loss: 0.5698 - acc: 0.7577 - val_loss: 0.5611 - val_acc: 0.7792\n",
      "Epoch 5/50\n",
      "718/718 [==============================] - 0s 135us/sample - loss: 0.5466 - acc: 0.7772 - val_loss: 0.5457 - val_acc: 0.7917\n",
      "Epoch 6/50\n",
      "718/718 [==============================] - 0s 141us/sample - loss: 0.5291 - acc: 0.7939 - val_loss: 0.5338 - val_acc: 0.7917\n",
      "Epoch 7/50\n",
      "718/718 [==============================] - 0s 126us/sample - loss: 0.5114 - acc: 0.8162 - val_loss: 0.5161 - val_acc: 0.8208\n",
      "Epoch 8/50\n",
      "718/718 [==============================] - 0s 152us/sample - loss: 0.4931 - acc: 0.8440 - val_loss: 0.5063 - val_acc: 0.8083\n",
      "Epoch 9/50\n",
      "718/718 [==============================] - 0s 190us/sample - loss: 0.4724 - acc: 0.8677 - val_loss: 0.4836 - val_acc: 0.8583\n",
      "Epoch 10/50\n",
      "718/718 [==============================] - 0s 363us/sample - loss: 0.4487 - acc: 0.8955 - val_loss: 0.4594 - val_acc: 0.8917\n",
      "Epoch 11/50\n",
      "718/718 [==============================] - 0s 126us/sample - loss: 0.4258 - acc: 0.9318 - val_loss: 0.4328 - val_acc: 0.9167\n",
      "Epoch 12/50\n",
      "718/718 [==============================] - 0s 147us/sample - loss: 0.4012 - acc: 0.9652 - val_loss: 0.4142 - val_acc: 0.9458\n",
      "Epoch 13/50\n",
      "718/718 [==============================] - 0s 136us/sample - loss: 0.3828 - acc: 0.9791 - val_loss: 0.4000 - val_acc: 0.9417\n",
      "Epoch 14/50\n",
      "718/718 [==============================] - 0s 149us/sample - loss: 0.3672 - acc: 0.9819 - val_loss: 0.3854 - val_acc: 0.9542\n",
      "Epoch 15/50\n",
      "718/718 [==============================] - 0s 140us/sample - loss: 0.3573 - acc: 0.9847 - val_loss: 0.3764 - val_acc: 0.9583\n",
      "Epoch 16/50\n",
      "718/718 [==============================] - 0s 138us/sample - loss: 0.3497 - acc: 0.9861 - val_loss: 0.3685 - val_acc: 0.9667\n",
      "Epoch 17/50\n",
      "718/718 [==============================] - 0s 137us/sample - loss: 0.3438 - acc: 0.9861 - val_loss: 0.3606 - val_acc: 0.9708\n",
      "Epoch 18/50\n",
      "718/718 [==============================] - 0s 190us/sample - loss: 0.3396 - acc: 0.9889 - val_loss: 0.3584 - val_acc: 0.9708\n",
      "Epoch 19/50\n",
      "718/718 [==============================] - 0s 88us/sample - loss: 0.3370 - acc: 0.9875 - val_loss: 0.3539 - val_acc: 0.9750\n",
      "Epoch 20/50\n",
      "718/718 [==============================] - 0s 155us/sample - loss: 0.3342 - acc: 0.9889 - val_loss: 0.3517 - val_acc: 0.9750\n",
      "Epoch 21/50\n",
      "718/718 [==============================] - 0s 125us/sample - loss: 0.3330 - acc: 0.9875 - val_loss: 0.3502 - val_acc: 0.9750\n",
      "Epoch 22/50\n",
      "718/718 [==============================] - 0s 91us/sample - loss: 0.3317 - acc: 0.9889 - val_loss: 0.3488 - val_acc: 0.9750\n",
      "Epoch 23/50\n",
      "718/718 [==============================] - 0s 333us/sample - loss: 0.3303 - acc: 0.9889 - val_loss: 0.3464 - val_acc: 0.9750\n",
      "Epoch 24/50\n",
      "718/718 [==============================] - 0s 361us/sample - loss: 0.3294 - acc: 0.9889 - val_loss: 0.3453 - val_acc: 0.9750\n",
      "Epoch 25/50\n",
      "718/718 [==============================] - 0s 304us/sample - loss: 0.3288 - acc: 0.9889 - val_loss: 0.3445 - val_acc: 0.9750\n",
      "Epoch 26/50\n",
      "718/718 [==============================] - 0s 235us/sample - loss: 0.3281 - acc: 0.9889 - val_loss: 0.3434 - val_acc: 0.9750\n",
      "Epoch 27/50\n",
      "718/718 [==============================] - 0s 154us/sample - loss: 0.3276 - acc: 0.9889 - val_loss: 0.3427 - val_acc: 0.9750\n",
      "Epoch 28/50\n",
      "718/718 [==============================] - 0s 128us/sample - loss: 0.3269 - acc: 0.9889 - val_loss: 0.3416 - val_acc: 0.9792\n",
      "Epoch 29/50\n",
      "718/718 [==============================] - 0s 154us/sample - loss: 0.3265 - acc: 0.9889 - val_loss: 0.3419 - val_acc: 0.9792\n",
      "Epoch 30/50\n",
      "718/718 [==============================] - 0s 166us/sample - loss: 0.3257 - acc: 0.9903 - val_loss: 0.3408 - val_acc: 0.9792\n",
      "Epoch 31/50\n",
      "718/718 [==============================] - 0s 100us/sample - loss: 0.3249 - acc: 0.9903 - val_loss: 0.3401 - val_acc: 0.9792\n",
      "Epoch 32/50\n",
      "718/718 [==============================] - 0s 122us/sample - loss: 0.3244 - acc: 0.9916 - val_loss: 0.3395 - val_acc: 0.9792\n",
      "Epoch 33/50\n",
      "718/718 [==============================] - 0s 138us/sample - loss: 0.3235 - acc: 0.9930 - val_loss: 0.3389 - val_acc: 0.9792\n",
      "Epoch 34/50\n",
      "718/718 [==============================] - 0s 200us/sample - loss: 0.3231 - acc: 0.9930 - val_loss: 0.3387 - val_acc: 0.9792\n",
      "Epoch 35/50\n",
      "718/718 [==============================] - 0s 352us/sample - loss: 0.3228 - acc: 0.9930 - val_loss: 0.3382 - val_acc: 0.9792\n",
      "Epoch 36/50\n",
      "718/718 [==============================] - 0s 129us/sample - loss: 0.3228 - acc: 0.9930 - val_loss: 0.3386 - val_acc: 0.9792\n",
      "Epoch 37/50\n",
      "718/718 [==============================] - 0s 95us/sample - loss: 0.3224 - acc: 0.9930 - val_loss: 0.3377 - val_acc: 0.9792\n",
      "Epoch 38/50\n",
      "718/718 [==============================] - 0s 130us/sample - loss: 0.3215 - acc: 0.9944 - val_loss: 0.3372 - val_acc: 0.9792\n",
      "Epoch 39/50\n",
      "718/718 [==============================] - 0s 151us/sample - loss: 0.3208 - acc: 0.9944 - val_loss: 0.3380 - val_acc: 0.9833\n",
      "Epoch 40/50\n",
      "718/718 [==============================] - 0s 145us/sample - loss: 0.3207 - acc: 0.9958 - val_loss: 0.3370 - val_acc: 0.9792\n",
      "Epoch 41/50\n",
      "718/718 [==============================] - 0s 118us/sample - loss: 0.3200 - acc: 0.9958 - val_loss: 0.3363 - val_acc: 0.9792\n",
      "Epoch 42/50\n",
      "718/718 [==============================] - 0s 135us/sample - loss: 0.3200 - acc: 0.9944 - val_loss: 0.3355 - val_acc: 0.9833\n",
      "Epoch 43/50\n",
      "718/718 [==============================] - 0s 177us/sample - loss: 0.3197 - acc: 0.9958 - val_loss: 0.3364 - val_acc: 0.9750\n",
      "Epoch 44/50\n",
      "718/718 [==============================] - 0s 125us/sample - loss: 0.3186 - acc: 0.9972 - val_loss: 0.3340 - val_acc: 0.9833\n",
      "Epoch 45/50\n",
      "718/718 [==============================] - 0s 90us/sample - loss: 0.3195 - acc: 0.9958 - val_loss: 0.3357 - val_acc: 0.9750\n",
      "Epoch 46/50\n",
      "718/718 [==============================] - 0s 136us/sample - loss: 0.3180 - acc: 0.9972 - val_loss: 0.3324 - val_acc: 0.9833\n",
      "Epoch 47/50\n",
      "718/718 [==============================] - 0s 123us/sample - loss: 0.3178 - acc: 0.9972 - val_loss: 0.3324 - val_acc: 0.9792\n",
      "Epoch 48/50\n",
      "718/718 [==============================] - 0s 158us/sample - loss: 0.3169 - acc: 0.9986 - val_loss: 0.3290 - val_acc: 0.9875\n",
      "Epoch 49/50\n",
      "718/718 [==============================] - 0s 159us/sample - loss: 0.3160 - acc: 1.0000 - val_loss: 0.3297 - val_acc: 0.9792\n",
      "Epoch 50/50\n",
      "718/718 [==============================] - 0s 156us/sample - loss: 0.3154 - acc: 1.0000 - val_loss: 0.3288 - val_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "# 5. Fit the training data.\n",
    "# 6. Evaluate your neural network model with the test data.\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    batch_size=50, \n",
    "                    epochs=50, \n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 718 samples, validate on 240 samples\n",
      "Epoch 1/5\n",
      "718/718 [==============================] - 0s 208us/sample - loss: 0.3150 - acc: 1.0000 - val_loss: 0.3300 - val_acc: 0.9833\n",
      "Epoch 2/5\n",
      "718/718 [==============================] - 0s 89us/sample - loss: 0.3147 - acc: 1.0000 - val_loss: 0.3294 - val_acc: 0.9833\n",
      "Epoch 3/5\n",
      "718/718 [==============================] - 0s 76us/sample - loss: 0.3147 - acc: 1.0000 - val_loss: 0.3297 - val_acc: 0.9833\n",
      "Epoch 4/5\n",
      "718/718 [==============================] - 0s 108us/sample - loss: 0.3146 - acc: 1.0000 - val_loss: 0.3290 - val_acc: 0.9833\n",
      "Epoch 5/5\n",
      "718/718 [==============================] - 0s 82us/sample - loss: 0.3145 - acc: 1.0000 - val_loss: 0.3301 - val_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    batch_size=64, \n",
    "                    epochs=5, \n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 164us/sample - loss: 0.3301 - acc: 0.9833\n",
      "0.33006079991658527 0.98333335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save your model as `tic-tac-toe.model`.\n",
    "model.save('tic-tac-toe.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make Predictions\n",
    "\n",
    "Now load your saved model and use it to make predictions on a few random rows in the test dataset. Check if the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown entries in loss dictionary: ['config', 'class_name']. Only expected following keys: ['output_1']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-988aed20e44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tic-tac-toe.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;31m# generate symbolic tensors).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1060\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2611\u001b[0m           \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m           \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2613\u001b[0;31m           cloning=self._cloning)\n\u001b[0m\u001b[1;32m   2614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2615\u001b[0m     \u001b[0;31m# In graph mode, if we had just set inputs and targets as symbolic tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;31m# Prepare list of loss functions, same size of model outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     self.loss_functions = training_utils.prepare_loss_functions(\n\u001b[0;32m--> 300\u001b[0;31m         self.loss, self.output_names)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mtarget_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_target_tensor_for_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_functions\u001b[0;34m(loss, output_names)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   \"\"\"\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_unexpected_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mcheck_for_unexpected_keys\u001b[0;34m(name, input_dict, expected_values)\u001b[0m\n\u001b[1;32m    590\u001b[0m     raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\n\u001b[1;32m    591\u001b[0m                      'following keys: {}'.format(name, list(unknown),\n\u001b[0;32m--> 592\u001b[0;31m                                                  expected_values))\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown entries in loss dictionary: ['config', 'class_name']. Only expected following keys: ['output_1']"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "new_model = tf.keras.models.load_model('tic-tac-toe.model')\n",
    "# make predictions\n",
    "predictor = new_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_inp_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-3bd8be62ffe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_inp_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_inp_test' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = new_model.predict(df_inp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown entries in loss dictionary: ['config', 'class_name']. Only expected following keys: ['output_1']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-4caf211de548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;31m# generate symbolic tensors).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1060\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2611\u001b[0m           \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2612\u001b[0m           \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2613\u001b[0;31m           cloning=self._cloning)\n\u001b[0m\u001b[1;32m   2614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2615\u001b[0m     \u001b[0;31m# In graph mode, if we had just set inputs and targets as symbolic tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;31m# Prepare list of loss functions, same size of model outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     self.loss_functions = training_utils.prepare_loss_functions(\n\u001b[0;32m--> 300\u001b[0;31m         self.loss, self.output_names)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mtarget_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_target_tensor_for_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_functions\u001b[0;34m(loss, output_names)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   \"\"\"\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_unexpected_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mcheck_for_unexpected_keys\u001b[0;34m(name, input_dict, expected_values)\u001b[0m\n\u001b[1;32m    590\u001b[0m     raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\n\u001b[1;32m    591\u001b[0m                      'following keys: {}'.format(name, list(unknown),\n\u001b[0;32m--> 592\u001b[0;31m                                                  expected_values))\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown entries in loss dictionary: ['config', 'class_name']. Only expected following keys: ['output_1']"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99342144e-01 6.57873403e-04]\n",
      " [1.19380718e-04 9.99880552e-01]\n",
      " [1.80484587e-03 9.98195112e-01]\n",
      " [9.99432981e-01 5.66939940e-04]\n",
      " [9.99929905e-01 7.00723467e-05]\n",
      " [2.66687851e-03 9.97333169e-01]\n",
      " [1.64287158e-05 9.99983549e-01]\n",
      " [3.97011749e-02 9.60298836e-01]\n",
      " [5.45998812e-02 9.45400119e-01]\n",
      " [9.75114048e-01 2.48858798e-02]\n",
      " [1.76409754e-04 9.99823630e-01]\n",
      " [9.57014927e-05 9.99904275e-01]\n",
      " [2.28290191e-05 9.99977112e-01]\n",
      " [1.48777947e-01 8.51222098e-01]\n",
      " [2.44816951e-03 9.97551858e-01]\n",
      " [2.34828144e-06 9.99997616e-01]\n",
      " [1.25528546e-03 9.98744726e-01]\n",
      " [3.54554795e-04 9.99645472e-01]\n",
      " [9.92122710e-01 7.87726603e-03]\n",
      " [9.96034443e-01 3.96560272e-03]\n",
      " [9.99981165e-01 1.88353915e-05]\n",
      " [2.56275634e-05 9.99974370e-01]\n",
      " [9.99295235e-01 7.04793725e-04]\n",
      " [1.02075443e-04 9.99897957e-01]\n",
      " [9.93153453e-01 6.84650661e-03]\n",
      " [3.93578596e-02 9.60642159e-01]\n",
      " [2.69043492e-04 9.99730885e-01]\n",
      " [3.72311741e-04 9.99627709e-01]\n",
      " [3.28050554e-01 6.71949506e-01]\n",
      " [1.53052201e-03 9.98469412e-01]\n",
      " [1.75854748e-05 9.99982357e-01]\n",
      " [3.42213811e-04 9.99657750e-01]\n",
      " [3.86095599e-05 9.99961376e-01]\n",
      " [1.26512095e-04 9.99873519e-01]\n",
      " [9.96716321e-01 3.28363036e-03]\n",
      " [9.99676228e-01 3.23835760e-04]\n",
      " [1.29448381e-04 9.99870539e-01]\n",
      " [3.71126895e-04 9.99628901e-01]\n",
      " [5.51518206e-05 9.99944806e-01]\n",
      " [9.96922314e-01 3.07763927e-03]\n",
      " [3.69599293e-04 9.99630332e-01]\n",
      " [1.54401074e-04 9.99845624e-01]\n",
      " [9.99924660e-01 7.53201384e-05]\n",
      " [9.99824822e-01 1.75197303e-04]\n",
      " [3.07571348e-02 9.69242871e-01]\n",
      " [6.52718306e-01 3.47281635e-01]\n",
      " [1.72078784e-03 9.98279214e-01]\n",
      " [4.48680512e-05 9.99955177e-01]\n",
      " [9.99264777e-01 7.35196110e-04]\n",
      " [9.99919653e-01 8.03962248e-05]\n",
      " [9.98928845e-01 1.07112806e-03]\n",
      " [9.99039054e-01 9.60943697e-04]\n",
      " [6.49092108e-05 9.99935031e-01]\n",
      " [2.28040433e-03 9.97719586e-01]\n",
      " [3.10955546e-03 9.96890485e-01]\n",
      " [1.93922142e-06 9.99998093e-01]\n",
      " [2.21597176e-04 9.99778450e-01]\n",
      " [9.95112360e-01 4.88765491e-03]\n",
      " [9.98276472e-01 1.72351603e-03]\n",
      " [1.07584638e-04 9.99892354e-01]\n",
      " [9.84991930e-05 9.99901533e-01]\n",
      " [7.70932878e-04 9.99229074e-01]\n",
      " [9.96795952e-01 3.20407143e-03]\n",
      " [5.99111700e-05 9.99940038e-01]\n",
      " [6.52455434e-04 9.99347508e-01]\n",
      " [1.35905657e-06 9.99998689e-01]\n",
      " [6.96349773e-04 9.99303699e-01]\n",
      " [9.99474943e-01 5.25071111e-04]\n",
      " [4.30097407e-06 9.99995708e-01]\n",
      " [5.75574959e-05 9.99942422e-01]\n",
      " [9.99613702e-01 3.86346393e-04]\n",
      " [9.99264181e-01 7.35861016e-04]\n",
      " [9.99838591e-01 1.61358737e-04]\n",
      " [4.95856293e-02 9.50414360e-01]\n",
      " [9.98513520e-01 1.48652052e-03]\n",
      " [4.82991491e-05 9.99951720e-01]\n",
      " [1.89762264e-02 9.81023788e-01]\n",
      " [9.93864954e-01 6.13500923e-03]\n",
      " [4.14550086e-05 9.99958515e-01]\n",
      " [9.97231424e-01 2.76852213e-03]\n",
      " [4.29291074e-04 9.99570668e-01]\n",
      " [1.36758565e-04 9.99863267e-01]\n",
      " [1.78309833e-03 9.98216927e-01]\n",
      " [9.84084792e-04 9.99015927e-01]\n",
      " [1.48968829e-05 9.99985099e-01]\n",
      " [1.17254844e-04 9.99882698e-01]\n",
      " [9.99200046e-01 7.99940666e-04]\n",
      " [9.97074246e-01 2.92570633e-03]\n",
      " [4.01639154e-05 9.99959826e-01]\n",
      " [4.39109790e-05 9.99956131e-01]\n",
      " [2.95851193e-03 9.97041523e-01]\n",
      " [9.95607913e-01 4.39210702e-03]\n",
      " [1.74135726e-04 9.99825895e-01]\n",
      " [9.61360335e-01 3.86397168e-02]\n",
      " [2.27944320e-03 9.97720540e-01]\n",
      " [1.92373840e-03 9.98076200e-01]\n",
      " [9.99915004e-01 8.50034194e-05]\n",
      " [1.32674256e-06 9.99998689e-01]\n",
      " [9.31026414e-04 9.99069035e-01]\n",
      " [1.83206193e-05 9.99981642e-01]\n",
      " [6.25998771e-04 9.99373972e-01]\n",
      " [2.61316145e-05 9.99973893e-01]\n",
      " [9.99736726e-01 2.63304624e-04]\n",
      " [2.42814749e-05 9.99975681e-01]\n",
      " [1.74746141e-02 9.82525349e-01]\n",
      " [6.13774289e-04 9.99386191e-01]\n",
      " [1.87144797e-05 9.99981284e-01]\n",
      " [1.41419310e-04 9.99858618e-01]\n",
      " [8.00247944e-05 9.99920011e-01]\n",
      " [5.65987080e-03 9.94340122e-01]\n",
      " [4.29503014e-03 9.95704949e-01]\n",
      " [2.42395373e-03 9.97576058e-01]\n",
      " [7.38353701e-03 9.92616475e-01]\n",
      " [9.95557368e-01 4.44260985e-03]\n",
      " [8.63512250e-05 9.99913692e-01]\n",
      " [4.52659413e-04 9.99547303e-01]\n",
      " [9.98678744e-01 1.32120028e-03]\n",
      " [1.07714033e-03 9.98922884e-01]\n",
      " [9.99870062e-01 1.29935899e-04]\n",
      " [9.99913931e-01 8.60410873e-05]\n",
      " [7.49838873e-05 9.99925017e-01]\n",
      " [1.06804579e-01 8.93195391e-01]\n",
      " [2.01147422e-03 9.97988582e-01]\n",
      " [9.99973059e-01 2.69384545e-05]\n",
      " [6.10933065e-01 3.89066964e-01]\n",
      " [2.01876173e-05 9.99979854e-01]\n",
      " [9.94211376e-01 5.78861590e-03]\n",
      " [3.15810794e-05 9.99968410e-01]\n",
      " [9.97038841e-01 2.96120229e-03]\n",
      " [2.53919061e-05 9.99974608e-01]\n",
      " [6.33613148e-04 9.99366343e-01]\n",
      " [5.85410511e-04 9.99414563e-01]\n",
      " [9.99802053e-01 1.97925110e-04]\n",
      " [2.01486127e-05 9.99979854e-01]\n",
      " [9.92975354e-01 7.02463230e-03]\n",
      " [1.47453075e-05 9.99985218e-01]\n",
      " [2.53004599e-02 9.74699497e-01]\n",
      " [2.52812984e-03 9.97471869e-01]\n",
      " [9.99966502e-01 3.34815850e-05]\n",
      " [1.47068193e-02 9.85293210e-01]\n",
      " [1.68509651e-02 9.83149052e-01]\n",
      " [2.29063735e-05 9.99977112e-01]\n",
      " [1.20943136e-04 9.99879003e-01]\n",
      " [3.85062391e-04 9.99614954e-01]\n",
      " [9.99671936e-01 3.28120048e-04]\n",
      " [4.20660572e-03 9.95793343e-01]\n",
      " [1.91349201e-04 9.99808729e-01]\n",
      " [9.96531844e-01 3.46809370e-03]\n",
      " [2.15180524e-04 9.99784887e-01]\n",
      " [7.52952183e-05 9.99924660e-01]\n",
      " [5.00549913e-06 9.99994993e-01]\n",
      " [9.97408211e-01 2.59183603e-03]\n",
      " [9.99813020e-01 1.86961435e-04]\n",
      " [9.96133685e-01 3.86630814e-03]\n",
      " [1.14760594e-03 9.98852372e-01]\n",
      " [4.31139115e-03 9.95688617e-01]\n",
      " [9.97167766e-01 2.83225230e-03]\n",
      " [4.77648020e-04 9.99522328e-01]\n",
      " [9.99974132e-01 2.59248754e-05]\n",
      " [4.98272898e-03 9.95017350e-01]\n",
      " [9.99376476e-01 6.23610336e-04]\n",
      " [9.97102439e-01 2.89756595e-03]\n",
      " [3.28366241e-06 9.99996662e-01]\n",
      " [3.60864833e-05 9.99963880e-01]\n",
      " [1.49957999e-03 9.98500466e-01]\n",
      " [8.84618657e-06 9.99991179e-01]\n",
      " [3.00236599e-04 9.99699831e-01]\n",
      " [2.95097241e-03 9.97049034e-01]\n",
      " [1.45111501e-03 9.98548806e-01]\n",
      " [1.14557391e-03 9.98854399e-01]\n",
      " [1.38729683e-05 9.99986172e-01]\n",
      " [9.97994304e-01 2.00569513e-03]\n",
      " [9.98476684e-01 1.52333663e-03]\n",
      " [1.42875602e-02 9.85712409e-01]\n",
      " [2.49890669e-04 9.99750078e-01]\n",
      " [1.09448978e-04 9.99890566e-01]\n",
      " [9.91651475e-01 8.34853482e-03]\n",
      " [1.44350954e-04 9.99855638e-01]\n",
      " [2.21679170e-06 9.99997735e-01]\n",
      " [9.99679208e-01 3.20850231e-04]\n",
      " [2.17235061e-06 9.99997854e-01]\n",
      " [9.04663932e-04 9.99095321e-01]\n",
      " [9.99689221e-01 3.10804840e-04]\n",
      " [9.98211622e-01 1.78834260e-03]\n",
      " [1.43379275e-05 9.99985695e-01]\n",
      " [5.21188776e-04 9.99478877e-01]\n",
      " [1.34779349e-01 8.65220726e-01]\n",
      " [3.16691399e-03 9.96833146e-01]\n",
      " [9.99663115e-01 3.36899160e-04]\n",
      " [3.07933005e-05 9.99969244e-01]\n",
      " [1.52806359e-04 9.99847174e-01]\n",
      " [9.92960632e-01 7.03930529e-03]\n",
      " [5.68433141e-04 9.99431551e-01]\n",
      " [3.07742797e-04 9.99692321e-01]\n",
      " [4.25656675e-04 9.99574363e-01]\n",
      " [1.30206374e-07 9.99999881e-01]\n",
      " [3.88261135e-04 9.99611795e-01]\n",
      " [5.90909505e-04 9.99409080e-01]\n",
      " [5.79328298e-05 9.99942064e-01]\n",
      " [9.99478638e-01 5.21367881e-04]\n",
      " [2.27301614e-03 9.97726977e-01]\n",
      " [6.30278373e-04 9.99369800e-01]\n",
      " [5.53559000e-03 9.94464457e-01]\n",
      " [7.33811129e-03 9.92661834e-01]\n",
      " [7.86249153e-03 9.92137492e-01]\n",
      " [3.37045669e-04 9.99662995e-01]\n",
      " [1.81748726e-06 9.99998212e-01]\n",
      " [5.47206670e-04 9.99452770e-01]\n",
      " [6.42409772e-02 9.35759008e-01]\n",
      " [7.12016423e-04 9.99288023e-01]\n",
      " [2.43496506e-05 9.99975681e-01]\n",
      " [7.89887446e-04 9.99210119e-01]\n",
      " [2.57328618e-04 9.99742687e-01]\n",
      " [4.95735985e-05 9.99950409e-01]\n",
      " [3.42284300e-04 9.99657750e-01]\n",
      " [1.35974446e-03 9.98640239e-01]\n",
      " [9.96952415e-01 3.04751168e-03]\n",
      " [9.99735415e-01 2.64553702e-04]\n",
      " [4.56130860e-04 9.99543846e-01]\n",
      " [9.97798979e-01 2.20099138e-03]\n",
      " [1.76354859e-03 9.98236418e-01]\n",
      " [9.94749725e-01 5.25023742e-03]\n",
      " [6.07534574e-08 9.99999881e-01]\n",
      " [1.49299143e-04 9.99850631e-01]\n",
      " [9.66144860e-01 3.38551141e-02]\n",
      " [9.99950051e-01 4.98983463e-05]\n",
      " [5.37465939e-06 9.99994636e-01]\n",
      " [1.00769720e-03 9.98992264e-01]\n",
      " [9.99781191e-01 2.18762463e-04]\n",
      " [7.75591354e-04 9.99224424e-01]\n",
      " [3.31170304e-05 9.99966860e-01]\n",
      " [8.38012493e-04 9.99161959e-01]\n",
      " [2.63976515e-03 9.97360170e-01]\n",
      " [1.29112720e-01 8.70887220e-01]\n",
      " [2.69018114e-04 9.99731004e-01]\n",
      " [3.01705836e-03 9.96982872e-01]\n",
      " [1.18085838e-04 9.99881864e-01]\n",
      " [5.33741666e-04 9.99466240e-01]\n",
      " [9.14358964e-07 9.99999046e-01]\n",
      " [2.37559862e-02 9.76244032e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pred = [np.argmax(predictor[x]) for x in range(x_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a45b1fa0344f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpredictions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpredictions_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/ironhack/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1476\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1477\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "'''\n",
    "1 means that player X will win,\n",
    "0 means that player Y will win or result will be a draw.\n",
    "'''\n",
    "\n",
    "#Prediction 25 results\n",
    "predictions = [np.argmax(predictor[i]) for i in range(y_test.shape[0])]\n",
    "print(predictions)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df['test'] = y_test.astype(int)\n",
    "\n",
    "correct = 0\n",
    "fail = 0\n",
    "\n",
    "for val in range(24):   \n",
    "    if predictions_df.loc[predictions_df[0] == predictions_df['test']]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "        \n",
    "print(f'In 25 attemps, the model was spon on {correct} times and failed {fail} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Improve Your Model\n",
    "\n",
    "Did your model achieve low loss (<0.1) and high accuracy (>0.95)? If not, try to improve your model.\n",
    "\n",
    "But how? There are so many things you can play with in Tensorflow and in the next challenge you'll learn about these things. But in this challenge, let's just do a few things to see if they will help.\n",
    "\n",
    "* Add more layers to your model. If the data are complex you need more layers. But don't use more layers than you need. If adding more layers does not improve the model performance you don't need additional layers.\n",
    "* Adjust the learning rate when you compile the model. This means you will create a custom `tf.keras.optimizers.Adam` instance where you specify the learning rate you want. Then pass the instance to `model.compile` as the optimizer.\n",
    "    * `tf.keras.optimizers.Adam` [reference](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n",
    "    * Don't worry if you don't understand what the learning rate does. You'll learn about it in the next challenge.\n",
    "* Adjust the number of epochs when you fit the training data to the model. Your model performance continues to improve as you train more epochs. But eventually it will reach the ceiling and the performance will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "model_opt = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0008), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "#              validation_data=(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 718 samples, validate on 240 samples\n",
      "Epoch 1/20\n",
      "718/718 [==============================] - 0s 646us/sample - loss: 0.6626 - acc: 0.6170 - val_loss: 0.6072 - val_acc: 0.6875\n",
      "Epoch 2/20\n",
      "718/718 [==============================] - 0s 139us/sample - loss: 0.6165 - acc: 0.6490 - val_loss: 0.5822 - val_acc: 0.7333\n",
      "Epoch 3/20\n",
      "718/718 [==============================] - 0s 158us/sample - loss: 0.5672 - acc: 0.7423 - val_loss: 0.5568 - val_acc: 0.7375\n",
      "Epoch 4/20\n",
      "718/718 [==============================] - 0s 152us/sample - loss: 0.5233 - acc: 0.7939 - val_loss: 0.5086 - val_acc: 0.8125\n",
      "Epoch 5/20\n",
      "718/718 [==============================] - 0s 157us/sample - loss: 0.4720 - acc: 0.8468 - val_loss: 0.4722 - val_acc: 0.8292\n",
      "Epoch 6/20\n",
      "718/718 [==============================] - 0s 202us/sample - loss: 0.4173 - acc: 0.9136 - val_loss: 0.4469 - val_acc: 0.8625\n",
      "Epoch 7/20\n",
      "718/718 [==============================] - 0s 160us/sample - loss: 0.3871 - acc: 0.9387 - val_loss: 0.4026 - val_acc: 0.9292\n",
      "Epoch 8/20\n",
      "718/718 [==============================] - 0s 189us/sample - loss: 0.3533 - acc: 0.9777 - val_loss: 0.3702 - val_acc: 0.9625\n",
      "Epoch 9/20\n",
      "718/718 [==============================] - 0s 205us/sample - loss: 0.3319 - acc: 0.9903 - val_loss: 0.3511 - val_acc: 0.9792\n",
      "Epoch 10/20\n",
      "718/718 [==============================] - 0s 163us/sample - loss: 0.3244 - acc: 0.9944 - val_loss: 0.3567 - val_acc: 0.9667\n",
      "Epoch 11/20\n",
      "718/718 [==============================] - 0s 163us/sample - loss: 0.3216 - acc: 0.9944 - val_loss: 0.3434 - val_acc: 0.9708\n",
      "Epoch 12/20\n",
      "718/718 [==============================] - 0s 210us/sample - loss: 0.3197 - acc: 0.9972 - val_loss: 0.3387 - val_acc: 0.9833\n",
      "Epoch 13/20\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.3174 - acc: 0.998 - 0s 227us/sample - loss: 0.3180 - acc: 0.9972 - val_loss: 0.3355 - val_acc: 0.9875\n",
      "Epoch 14/20\n",
      "718/718 [==============================] - 0s 282us/sample - loss: 0.3174 - acc: 0.9972 - val_loss: 0.3341 - val_acc: 0.9875\n",
      "Epoch 15/20\n",
      "718/718 [==============================] - 0s 281us/sample - loss: 0.3168 - acc: 0.9972 - val_loss: 0.3309 - val_acc: 0.9875\n",
      "Epoch 16/20\n",
      "718/718 [==============================] - 0s 288us/sample - loss: 0.3162 - acc: 0.9986 - val_loss: 0.3305 - val_acc: 0.9833\n",
      "Epoch 17/20\n",
      "718/718 [==============================] - 0s 294us/sample - loss: 0.3158 - acc: 0.9986 - val_loss: 0.3340 - val_acc: 0.9833\n",
      "Epoch 18/20\n",
      "718/718 [==============================] - 0s 129us/sample - loss: 0.3155 - acc: 0.9986 - val_loss: 0.3323 - val_acc: 0.9833\n",
      "Epoch 19/20\n",
      "718/718 [==============================] - 0s 153us/sample - loss: 0.3153 - acc: 0.9986 - val_loss: 0.3294 - val_acc: 0.9875\n",
      "Epoch 20/20\n",
      "718/718 [==============================] - 0s 135us/sample - loss: 0.3151 - acc: 0.9986 - val_loss: 0.3284 - val_acc: 0.9917\n"
     ]
    }
   ],
   "source": [
    "history = model_opt.fit(x_train, \n",
    "                    y_train, \n",
    "                    batch_size=50, \n",
    "                    epochs=20, \n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_training(history, metrics: list = ('loss',), figsize: tuple = (12, 5)):\n",
    "    \"\"\"\n",
    "    plots training selected metrics for every batch\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history[metrics[0]]) + 1)\n",
    "\n",
    "    fig, ax_arr = plt.subplots(1, len(metrics), figsize=figsize)\n",
    "\n",
    "    if not isinstance(ax_arr, np.ndarray):\n",
    "        ax_arr = np.array(ax_arr).reshape(1, )\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax_arr[i].plot(epochs, history.history[metric], color='k', linestyle='solid', label=metric, linewidth=2)\n",
    "        ax_arr[i].plot(epochs, history.history[f\"val_{metric}\"], color='r', linestyle='dotted',\n",
    "                       label=f'validation {metric}')\n",
    "        ax_arr[i].set_ylabel(metric)\n",
    "        ax_arr[i].set_xlabel('epochs')\n",
    "        ax_arr[i].grid()\n",
    "        ax_arr[i].legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training(history, metrics=['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which approach(es) did you find helpful to improve your model performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack]",
   "language": "python",
   "name": "conda-env-ironhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
